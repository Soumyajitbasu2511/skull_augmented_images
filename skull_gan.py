# -*- coding: utf-8 -*-
"""skull_gan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jXbyXaqqw27M1g547x2CA17IDR9C_RBK
"""

import os

import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from keras import layers
from keras.preprocessing.image import ImageDataGenerator

# Generate random noise as input for the generator
def generate_random_noise(batch_size, latent_dim):
    return np.random.normal(0, 1, (batch_size, latent_dim))

# Build the generator model
def build_generator(latent_dim):
    model = keras.Sequential()
    model.add(layers.Dense(256, input_dim=latent_dim, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(1024, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(512 * 512 * 3, activation='sigmoid'))  # Output layer for (156, 156, 3) image
    model.add(layers.Reshape((512, 512, 3)))  # Assuming generating (156, 156, 3) images
    return model


# Build the discriminator model
def build_discriminator(img_shape):
    model = keras.Sequential()
    model.add(layers.Flatten(input_shape=img_shape))
    model.add(layers.Dense(1024, activation='relu'))
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

# Build the GAN model
def build_gan(generator, discriminator):
    discriminator.trainable = False
    model = keras.Sequential()
    model.add(generator)
    model.add(discriminator)
    return model

# Compile the discriminator
def compile_discriminator(discriminator):
    discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])

# Compile the GAN
def compile_gan(gan):
    gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))
    #gan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(learning_rate=0.001))

# Train the GAN
def train_gan(generator, discriminator, gan, epochs, batch_size, latent_dim, data_generator, data_path, save_path='generated_images4/'):
    os.makedirs(save_path, exist_ok=True)

    for epoch in range(epochs):
        # Load real data from the directory
        real_data = data_generator.flow(
        x=np.array([plt.imread(data_path)]),  # Read and convert the image to a NumPy array
        batch_size=batch_size,
        shuffle=True)

        # Iterate over batches of real data
        for batch in real_data:
            real_images = batch
            break  # Only one batch for simplicity

        noise = generate_random_noise(batch_size, latent_dim)
        generated_images = generator.predict(noise)

        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))

        # Train discriminator on real and fake data separately
        d_loss_real = discriminator.train_on_batch(real_images, real_labels)
        d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)

        # Calculate discriminator loss
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train the generator (GAN) to fool the discriminator
        noise = generate_random_noise(batch_size, latent_dim)
        valid_labels = np.ones((batch_size, 1))
        g_loss = gan.train_on_batch(noise, valid_labels)

        # Print progress and save generated images
        print(f"Epoch {epoch + 1}, D Loss: {d_loss[0]}, G Loss: {g_loss}")
        if epoch % 2 == 0:
            save_generated_images(generator, epoch, latent_dim)

# Save generated images during training
def save_generated_images(generator, epoch, latent_dim, examples=10, save_path='generated_images4/'):
    noise = generate_random_noise(examples, latent_dim)
    generated_images = generator.predict(noise)
    generated_images = (generated_images + 1) / 2.0  # Rescale images to [0, 1]

    for i in range(examples):
        plt.subplot(1, examples, i + 1)
        plt.imshow(generated_images[i, :, :, 0],cmap='gray')
        plt.axis('off')

    plt.savefig(f"{save_path}gan_generated_image_epoch_{epoch}.png")
    plt.close()

# Example usage
latent_dim = 100
img_shape = (512,512,3)  # Assuming generating 156x156 grayscale images
epochs = 500
batch_size = 1
data_path = '/content/drive/MyDrive/skull_aug_4/_0_1227.png'  # Replace with the path to your data directory

# Create an ImageDataGenerator for loading and preprocessing images
data_generator = ImageDataGenerator(rescale=1./255)

# Build and compile the discriminator
discriminator = build_discriminator(img_shape)
compile_discriminator(discriminator)

# Build the generator
generator = build_generator(latent_dim)

# Build the GAN model
discriminator.trainable = False  # Freeze discriminator during GAN training
gan = build_gan(generator, discriminator)

# Compile the GAN
compile_gan(gan)

# Train the GAN
train_gan(generator, discriminator, gan, epochs, batch_size, latent_dim, data_generator, data_path)

from google.colab import drive
drive.mount('/content/drive')

import shutil

# Specify the path to the directory you want to delete
directory_path = '/content/generated_images4'

# Use shutil.rmtree to delete the directory and its contents
shutil.rmtree(directory_path)

